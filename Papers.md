## Papers

## Survey Papers

[[A Practical Survey on Faster and Lighter Transformers]] : Popular approaches to make Transformers fasters and lighter by providing a comprehensive explanation of the methods' strengths, limitations and underlying assumptions
## Uncategorised

[Matryoshka Embeddings](/Matryoshka%20Embeddings.md): Training embedding models that are able to work at a variety of different embedding dimensions

[Language Models Are Unsupervised Multitask Learners](/Language%20Models%20Are%20Unsupervised%20Multitask%20Learners.md) : How GPT-2 challenged a traditional paradigm of pre-train -> fine-tune on task with its auto-regressive prompting ability

[A Comprehensive Overview of Large Language Models](/A%20Comprehensive%20Overview%20of%20Large%20Language%20Models.md) : A walkthrough of the key ideas and concepts around large language models

[Improving Language Understanding by Generative Pre-Training](/Improving%20Language%20Understanding%20by%20Generative%20Pre-Training.md) : How GPT-1 helped to bring forth a huge revolution in the NLP space by showing efficient transfer learning abilities

[[ORPO Monolithic Preference Optimization without Reference Model]] : Using the odds ratio of LLM output generated directly to fine-tune a model. This combines the SFT and RLHF stages in a single stage

[[LoRA Learns Less and Forgets Less]] : An ablation study of LoRA's performance as compared to a full fine-tune when it comes to instruction fine-tuning and continued pre-training.
