
LLMs are a new form of language models which have a tremendous ability to understand, generate and interpret human language. They are incredibly useful because they are able to perform many tasks without having handcrafted rules or specialised datasets.

Fundamentally, these are models that are trained to understand, generate and respond to human-like text. They are considered large because their parameters tend to scale up to the hundreds of billions. They are trained in a self-supervised manner by being made to predict the next token.

They utilise a [[Transformer Architecture]] which allows them to pay selective attention to different parts of the input when making predictions. Typically training for these models happens in 3 stages

1. Pretraining: This is

Instruct language models have dominated the public space. There are a good amount of models which mimic the style of ChatGPT. There are a few key algorithms to know which are used to train these models

1. [[RLHF]]
